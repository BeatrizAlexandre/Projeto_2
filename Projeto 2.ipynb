{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificando Automático de Sentimento \n",
    "\n",
    "## Por: Beatriz Alexandre e Vitória Almeida\n",
    "\n",
    "#### Marca escolhida: Lindt\n",
    "\n",
    "Fomos contratadas pela empresa Lindt, pára analisar os tweets feitos por seus clientessobre sua empresa. Para isso criaremos um programa que irá analisae as mensagens disponiveis como \"relevante\" ou \"irrelevante\". Os twers que forem negativos irão disparar um foco na área de marketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl  \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colocando os dados em um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = pd.read_excel('tweets_lindt.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Positivo/Negativo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lindt chocolate is so overrated</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@elgliko hear that pitter patter in the back g...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt @statutory_boy: the guy was addicted to dru...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@andreyasasylum @yourmomsuckstho hey, can you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great job by @noticgroup gms. come and see the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento Relevante  \\\n",
       "0                    lindt chocolate is so overrated         1   \n",
       "1  @elgliko hear that pitter patter in the back g...         0   \n",
       "2  rt @statutory_boy: the guy was addicted to dru...         0   \n",
       "3  @andreyasasylum @yourmomsuckstho hey, can you ...         1   \n",
       "4  great job by @noticgroup gms. come and see the...         1   \n",
       "\n",
       "  Positivo/Negativo  \n",
       "0                 0  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3                 1  \n",
       "4                 1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando os tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIMPAR TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>Positivo/Negativo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lindt chocolate is so overrated</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elgliko hear that pitter patter in the back gr...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt statutoryboy the guy was addicted to drugs,...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>andreyasasylum yourmomsuckstho hey, can you pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great job by noticgroup gms. come and see them...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento Relevante  \\\n",
       "0                    lindt chocolate is so overrated         1   \n",
       "1  elgliko hear that pitter patter in the back gr...         0   \n",
       "2  rt statutoryboy the guy was addicted to drugs,...         0   \n",
       "3  andreyasasylum yourmomsuckstho hey, can you pl...         1   \n",
       "4  great job by noticgroup gms. come and see them...         1   \n",
       "\n",
       "  Positivo/Negativo  \n",
       "0                 0  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3                 1  \n",
       "4                 1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.Treinamento = tw.Treinamento.str.replace('@', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace(':', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('_', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('%', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('&', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace(',', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('#', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace(';', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('.', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('$', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('(', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace(')', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('*', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('/', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('?', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('+', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('-', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace(']', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('[', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('=', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('^', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('!', '')\n",
    "tw.Treinamento = tw.Treinamento.str.replace('\"', '')\n",
    "\n",
    "tw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dar espaço nos emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lindt chocolate is so overrated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elgliko hear that pitter patter in the back gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt statutoryboy the guy was addicted to drugs,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>andreyasasylum yourmomsuckstho hey, can you pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great job by noticgroup gms. come and see them...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento Relevante\n",
       "0                    lindt chocolate is so overrated         1\n",
       "1  elgliko hear that pitter patter in the back gr...         0\n",
       "2  rt statutoryboy the guy was addicted to drugs,...         0\n",
       "3  andreyasasylum yourmomsuckstho hey, can you pl...         1\n",
       "4  great job by noticgroup gms. come and see them...         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_espaco_emoji = []\n",
    "data = tw[\"Treinamento\"]\n",
    "for tweet in data:\n",
    "    t = ''\n",
    "    for palavra in tweet:\n",
    "        if palavra in UNICODE_EMOJI:\n",
    "            t = t+ \" \" + palavra + \" \"\n",
    "        else:\n",
    "            t += palavra\n",
    "    tweets_espaco_emoji.append(t)\n",
    "\n",
    "tw_limpo =  pd.DataFrame()\n",
    "tw_limpo['Treinamento'] = tweets_espaco_emoji\n",
    "tw_limpo['Relevante'] = tw['Relevante']\n",
    "\n",
    "tw_limpo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista de todas as palavras dos tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pala = []\n",
    "\n",
    "for i in tw_limpo['Treinamento']:\n",
    "    frase = i.split()\n",
    "    for p in frase:\n",
    "        if p not in pala:\n",
    "            pala.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contar quantas vezes aparace o 1 e o 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contador\n",
    "Rel = 0\n",
    "Irre = 0 \n",
    "\n",
    "for l in range(len(tw_limpo)):\n",
    "    linha = tw_limpo[\"Treinamento\"][l].split(' ')\n",
    "    for i in linha:\n",
    "        if tw_limpo[\"Treinamento\"][l] == 1:\n",
    "            Rel += 1\n",
    "        else:\n",
    "            Irre += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicionários para calcular a frequencia de relevantes e irrelevantes:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lindt': 1,\n",
       " 'chocolate': 1,\n",
       " 'is': 1,\n",
       " 'so': 1,\n",
       " 'overrated': 1,\n",
       " 'elgliko': 1,\n",
       " 'hear': 1,\n",
       " 'that': 1,\n",
       " 'pitter': 1,\n",
       " 'patter': 1,\n",
       " 'in': 1,\n",
       " 'the': 1,\n",
       " 'back': 1,\n",
       " 'ground': 1,\n",
       " 'every': 1,\n",
       " 'often': 1,\n",
       " 'that’s': 1,\n",
       " 'me': 1,\n",
       " 'your': 1,\n",
       " 'costco': 1,\n",
       " 'where': 1,\n",
       " 'i': 1,\n",
       " 'am': 1,\n",
       " 'living': 1,\n",
       " 'off': 1,\n",
       " 'a': 1,\n",
       " 'rounded': 1,\n",
       " 'diet': 1,\n",
       " 'of': 1,\n",
       " 'slim': 1,\n",
       " 'jim’s': 1,\n",
       " 'amp': 1,\n",
       " 'reading': 1,\n",
       " 'romance': 1,\n",
       " 'novels': 1,\n",
       " 'by': 1,\n",
       " '6pack': 1,\n",
       " 'and': 1,\n",
       " 'having': 1,\n",
       " 'my': 1,\n",
       " 'pick': 1,\n",
       " 'giant': 1,\n",
       " 'pile': 1,\n",
       " 'dog': 1,\n",
       " 'beds': 1,\n",
       " 'for': 1,\n",
       " 'some': 1,\n",
       " 'shuteye': 1,\n",
       " 'you’ll': 1,\n",
       " 'only': 1,\n",
       " 'find': 1,\n",
       " 'rt': 1,\n",
       " 'statutoryboy': 1,\n",
       " 'guy': 1,\n",
       " 'was': 1,\n",
       " 'addicted': 1,\n",
       " 'to': 1,\n",
       " 'drugs': 1,\n",
       " 'his': 1,\n",
       " 'life': 1,\n",
       " 'changed': 1,\n",
       " 'after': 1,\n",
       " 'he': 1,\n",
       " 'met': 1,\n",
       " 'this': 1,\n",
       " 'woman': 1,\n",
       " 'god': 1,\n",
       " 'bless': 1,\n",
       " '❤': 1,\n",
       " '👊': 1,\n",
       " '🙏': 1,\n",
       " 'httpstcodcnehu…': 1,\n",
       " 'andreyasasylum': 1,\n",
       " 'yourmomsuckstho': 1,\n",
       " 'hey': 1,\n",
       " 'can': 1,\n",
       " 'you': 1,\n",
       " 'please': 1,\n",
       " 'pass': 1,\n",
       " 'last': 1,\n",
       " 'dark': 1,\n",
       " 'great': 1,\n",
       " 'job': 1,\n",
       " 'noticgroup': 1,\n",
       " 'gms': 1,\n",
       " 'come': 1,\n",
       " 'see': 1,\n",
       " 'them': 1,\n",
       " 'make': 1,\n",
       " 'sure': 1,\n",
       " 'ask': 1,\n",
       " 'ball': 1,\n",
       " 'signaturesolutions': 1,\n",
       " 'wearecbr': 1,\n",
       " 'httpstcoynsmdfcxaj': 1,\n",
       " 'blacktwittasa': 1,\n",
       " 'lmao': 1,\n",
       " 'guys': 1,\n",
       " \"i'm\": 1,\n",
       " 'screaming': 1,\n",
       " '🤣': 1,\n",
       " 'httpstcosx7w2cfmvx': 1,\n",
       " 'fuck': 1,\n",
       " 'want': 1,\n",
       " 'candy': 1,\n",
       " 'or': 1,\n",
       " 'going': 1,\n",
       " '✈': 1,\n",
       " '️': 1,\n",
       " 'spend': 1,\n",
       " 'all': 1,\n",
       " 'money': 1,\n",
       " 'at': 1,\n",
       " 'store': 1,\n",
       " 'lindtzwane': 1,\n",
       " 'thank': 1,\n",
       " 'misinterpret': 1,\n",
       " 'love': 1,\n",
       " 'sex': 1,\n",
       " '💵': 1,\n",
       " 'reliability': 1,\n",
       " 'patriarchy': 1,\n",
       " 'just': 1,\n",
       " 'ruined': 1,\n",
       " 'whole': 1,\n",
       " 'meaning': 1,\n",
       " 'true': 1,\n",
       " 'reebok': 1,\n",
       " 'emailed': 1,\n",
       " 'collab': 1,\n",
       " 'posts': 1,\n",
       " 'with': 1,\n",
       " 'their': 1,\n",
       " 'shoes': 1,\n",
       " 'nothing': 1,\n",
       " 'else': 1,\n",
       " 'matters': 1,\n",
       " 'today': 1,\n",
       " 'except': 1,\n",
       " 'sent': 1,\n",
       " 'ten': 1,\n",
       " 'truffle': 1,\n",
       " 'bags': 1,\n",
       " 'who': 1,\n",
       " 'wants': 1,\n",
       " 'turn': 1,\n",
       " 'up': 1,\n",
       " 'w': 1,\n",
       " '😥': 1,\n",
       " 'news': 1,\n",
       " 'coffee': 1,\n",
       " 'irish': 1,\n",
       " 'cream': 1,\n",
       " 'hazelnut': 1,\n",
       " 'gianduja': 1,\n",
       " \"lindor's\": 1,\n",
       " 'discontinued': 1,\n",
       " 'get': 1,\n",
       " \"what's\": 1,\n",
       " 'left': 1,\n",
       " 'while': 1,\n",
       " 'httpstconcphz2ypb7': 1,\n",
       " 'httpstcohfslcab317': 1,\n",
       " 'earwolf': 1,\n",
       " 'mattgourley': 1,\n",
       " 'tvsandydaly': 1,\n",
       " 'new': 1,\n",
       " 'august': 1,\n",
       " 'whooohoooo': 1,\n",
       " 'finally': 1,\n",
       " 'end': 1,\n",
       " 'sufferings': 1,\n",
       " 'anewnamehere': 1,\n",
       " 'verelaurent': 1,\n",
       " \"it's\": 1,\n",
       " 'museum': 1,\n",
       " \"haven't\": 1,\n",
       " 'been': 1,\n",
       " 'there': 1,\n",
       " 'yet': 1,\n",
       " 'but': 1,\n",
       " 'one': 1,\n",
       " 'day': 1,\n",
       " 'will': 1,\n",
       " 'lol': 1,\n",
       " 'how': 1,\n",
       " 'much': 1,\n",
       " 'out': 1,\n",
       " '10': 1,\n",
       " 'would': 1,\n",
       " 'rate': 1,\n",
       " '70': 1,\n",
       " 'darkchocolate': 1,\n",
       " 'version': 1,\n",
       " 'lindtexcellence': 1,\n",
       " 'blocks': 1,\n",
       " 'httpstcoaeppg6mlu4': 1,\n",
       " 'gemsatwork': 1,\n",
       " 'real': 1,\n",
       " '😍': 1,\n",
       " 'we': 1,\n",
       " 'are': 1,\n",
       " 'over': 1,\n",
       " 'moon': 1,\n",
       " 'be': 1,\n",
       " 'working': 1,\n",
       " 'again': 1,\n",
       " 'what': 1,\n",
       " 'an': 1,\n",
       " 'amazing': 1,\n",
       " 'way': 1,\n",
       " 'distract': 1,\n",
       " 'us': 1,\n",
       " 'from': 1,\n",
       " 'postsumme…': 1,\n",
       " 'curiouspeterg': 1,\n",
       " 'unicorns': 1,\n",
       " 'fake': 1,\n",
       " 'giraffes': 1,\n",
       " 'like': 1,\n",
       " 'whats': 1,\n",
       " 'more': 1,\n",
       " 'believable': 1,\n",
       " 'horse': 1,\n",
       " 'horn': 1,\n",
       " 'leopardmoosecamel': 1,\n",
       " 'wi…': 1,\n",
       " 'crumbschester': 1,\n",
       " '🍰': 1,\n",
       " 'heading': 1,\n",
       " 'makersmarket': 1,\n",
       " 'chester': 1,\n",
       " 'month': 1,\n",
       " 'we’ve': 1,\n",
       " 'hard': 1,\n",
       " 'on': 1,\n",
       " 'tasty': 1,\n",
       " 'treats': 1,\n",
       " 'lately': 1,\n",
       " 'amp…': 1,\n",
       " 'kira': 1,\n",
       " 'asked': 1,\n",
       " 'buy': 1,\n",
       " 'her': 1,\n",
       " 'chocolates': 1,\n",
       " 'has': 1,\n",
       " 'mix': 1,\n",
       " 'shop': 1,\n",
       " 'take': 1,\n",
       " 'as': 1,\n",
       " 'long': 1,\n",
       " 'fit': 1,\n",
       " 'it': 1,\n",
       " 'box': 1,\n",
       " 'level': 1,\n",
       " 'asian': 1,\n",
       " 'httpstco2lumsa7bdp': 1,\n",
       " 'piwaaikoxo': 1,\n",
       " 'nope': 1,\n",
       " '😂': 1,\n",
       " 'baby': 1,\n",
       " 'previous': 1,\n",
       " 'post': 1,\n",
       " 'sensational': 1,\n",
       " 'taste': 1,\n",
       " 'fruit': 1,\n",
       " '|': 1,\n",
       " 'sensation': 1,\n",
       " 'range': 1,\n",
       " 'httpstcoqqwz9gqtvp': 1,\n",
       " 'little': 1,\n",
       " 'things': 1,\n",
       " 'best': 1,\n",
       " 'happywifehappylife': 1,\n",
       " 'httpstcokmdb4mwtht': 1,\n",
       " 'co': 1,\n",
       " 'sexy': 1,\n",
       " 'intriguing': 1,\n",
       " 'httpstcon5adazxwoa': 1,\n",
       " 'serenbdavies': 1,\n",
       " 'do': 1,\n",
       " 'people': 1,\n",
       " 'have': 1,\n",
       " '1': 1,\n",
       " 'square': 1,\n",
       " 'salted': 1,\n",
       " 'caramel': 1,\n",
       " 'stop': 1,\n",
       " 'asking': 1,\n",
       " 'friend': 1,\n",
       " 'bonga': 1,\n",
       " 'put': 1,\n",
       " 'seatbelts': 1,\n",
       " 'when': 1,\n",
       " 'driving': 1,\n",
       " 'i’ve': 1,\n",
       " 'informed': 1,\n",
       " 'old': 1,\n",
       " 'passed': 1,\n",
       " 'away': 1,\n",
       " 'yesterday': 1,\n",
       " 'morning': 1,\n",
       " 'after…': 1,\n",
       " 'lindtchoc': 1,\n",
       " 'ohloowatoscene': 1,\n",
       " 'very': 1,\n",
       " 'iamremy': 1,\n",
       " 'gives': 1,\n",
       " 'u': 1,\n",
       " 'look': 1,\n",
       " '👇': 1,\n",
       " 'trying': 1,\n",
       " 'emotional': 1,\n",
       " 'inspirational': 1,\n",
       " 'to…': 1,\n",
       " 'lindtm': 1,\n",
       " 'kayepooot': 1,\n",
       " 'as…': 1,\n",
       " 'blackwave10': 1,\n",
       " 'mrsuitup': 1,\n",
       " 'isa': 1,\n",
       " 'tough': 1,\n",
       " 'subjectmany': 1,\n",
       " 'faileddismally': 1,\n",
       " 'cases': 1,\n",
       " 'scares': 1,\n",
       " 'hell': 1,\n",
       " 'outa': 1,\n",
       " 'try': 1,\n",
       " 'i…': 1,\n",
       " 'spottersuper': 1,\n",
       " 'requirements': 1,\n",
       " 'expected': 1,\n",
       " 'man': 1,\n",
       " 'marry': 1,\n",
       " 'imo': 1,\n",
       " 'state': 1,\n",
       " 'nigeria': 1,\n",
       " 'httpstconopv5hyrsy': 1,\n",
       " 'gabrielle': 1,\n",
       " 'made': 1,\n",
       " 'cookies': 1,\n",
       " \"i've\": 1,\n",
       " 'ever': 1,\n",
       " 'eaten': 1,\n",
       " 'other': 1,\n",
       " 'check': 1,\n",
       " 'here': 1,\n",
       " '🍪': 1,\n",
       " 'httpstcoimgms6fsdr': 1,\n",
       " 'vickerysec': 1,\n",
       " 'talking': 1,\n",
       " 'about': 1,\n",
       " 'australian': 1,\n",
       " 'security': 1,\n",
       " 'border': 1,\n",
       " 'control': 1,\n",
       " 'don’t': 1,\n",
       " 'they': 1,\n",
       " 'usually': 1,\n",
       " 'into': 1,\n",
       " 'country': 1,\n",
       " 'especially': 1,\n",
       " 'checks': 1,\n",
       " 'hazardous': 1,\n",
       " 'materials': 1,\n",
       " 'parents': 1,\n",
       " 'got': 1,\n",
       " 'trouble': 1,\n",
       " 'once': 1,\n",
       " 'not': 1,\n",
       " 'declared': 1,\n",
       " 'two': 1,\n",
       " 'easter': 1,\n",
       " 'bunnies': 1,\n",
       " 'laralafleur': 1,\n",
       " 'sea': 1,\n",
       " 'salt': 1,\n",
       " 'also': 1,\n",
       " 'vego': 1,\n",
       " 'vegan': 1,\n",
       " 'hazelnuts': 1,\n",
       " 'too': 1,\n",
       " 'imagine': 1,\n",
       " '😩': 1,\n",
       " 'franyjacobs': 1,\n",
       " 'never': 1,\n",
       " 'knew': 1,\n",
       " 'shemale': 1,\n",
       " 'until': 1,\n",
       " 'filled': 1,\n",
       " 'dm': 1,\n",
       " 'women': 1,\n",
       " 'penises': 1,\n",
       " 'worst': 1,\n",
       " 'part': 1,\n",
       " 'opening': 1,\n",
       " 'line': 1,\n",
       " 'hi': 1,\n",
       " 'know': 1,\n",
       " 'no': 1,\n",
       " 'oh': 1,\n",
       " 'pictures': 1,\n",
       " 'kiandymundi': 1,\n",
       " 'blgsteve': 1,\n",
       " 'positivegpmain': 1,\n",
       " 'serinide': 1,\n",
       " 'r': 1,\n",
       " 'rly': 1,\n",
       " 'tryna': 1,\n",
       " 'tell': 1,\n",
       " 'both': 1,\n",
       " 'kinder': 1,\n",
       " 'shit': 1,\n",
       " 'tier': 1,\n",
       " 'jhusblaze': 1,\n",
       " 'year': 1,\n",
       " '2050': 1,\n",
       " '\\U0001f91f': 1,\n",
       " 'httpstcowhosurtrfy': 1,\n",
       " '90': 1,\n",
       " 'cocoa': 1,\n",
       " 'thing': 1,\n",
       " 'kelvinmo': 1,\n",
       " 'you’re': 1,\n",
       " 'hug': 1,\n",
       " 'kiss': 1,\n",
       " 'mom': 1,\n",
       " 'lindor': 1,\n",
       " 'mint': 1,\n",
       " 'small': 1,\n",
       " 'bar': 1,\n",
       " 'tescofood': 1,\n",
       " '😋': 1,\n",
       " 'lindtchocolate': 1,\n",
       " 'lindtstagram': 1,\n",
       " 'lindtlindor': 1,\n",
       " 'mintchocolate': 1,\n",
       " 'chocolatetruffle': 1,\n",
       " 'newfood': 1,\n",
       " 'sweets…': 1,\n",
       " 'httpstcosxww903pfq': 1,\n",
       " 'itzthulz': 1,\n",
       " 'time': 1,\n",
       " 'academic': 1,\n",
       " 'damage': 1,\n",
       " '😠': 1,\n",
       " 'firstdownfr': 1,\n",
       " 'shaquem': 1,\n",
       " 'griffin': 1,\n",
       " 'x': 1,\n",
       " 'odell': 1,\n",
       " 'beckham': 1,\n",
       " 'jr': 1,\n",
       " 'httpstcosxms0m2r3p': 1,\n",
       " 'whoever': 1,\n",
       " 'keeps': 1,\n",
       " 'overfilling': 1,\n",
       " 'truffles': 1,\n",
       " 'commercials': 1,\n",
       " 'gonna': 1,\n",
       " 'himself': 1,\n",
       " 'fucking': 1,\n",
       " 'fired': 1,\n",
       " 'lovebedford': 1,\n",
       " 'mintwhat': 1,\n",
       " 'could': 1,\n",
       " '😱': 1,\n",
       " 'arcadia': 1,\n",
       " 'sweet': 1,\n",
       " 'httpstcoa47rraqkho': 1,\n",
       " 'dreamhousex': 1,\n",
       " 'backyard': 1,\n",
       " 'pool': 1,\n",
       " 'paradise': 1,\n",
       " '🏊': 1,\n",
       " 'httpstcoq9ga9jvlnp': 1,\n",
       " 'someone': 1,\n",
       " 'something': 1,\n",
       " '😭': 1,\n",
       " 'need': 1,\n",
       " 'httpstcot9i6rb4gff': 1,\n",
       " 'lovefay': 1,\n",
       " 'face': 1,\n",
       " \"can't\": 1,\n",
       " 'wait': 1,\n",
       " 'ours': 1,\n",
       " 'ascentoffices': 1,\n",
       " 'lindtuk': 1,\n",
       " 'freebiesatwork': 1,\n",
       " 'freebies': 1,\n",
       " '\\U0001f970\\U0001f973': 1,\n",
       " 'tesco': 1,\n",
       " 'its': 1,\n",
       " 'xmas': 1,\n",
       " 'confectionary': 1,\n",
       " 'aisle': 1,\n",
       " 'fully': 1,\n",
       " 'stocked': 1,\n",
       " 'early': 1,\n",
       " 'reindeer': 1,\n",
       " 'needs': 1,\n",
       " '120ct': 1,\n",
       " 'httpstcoelaf5uh9ta': 1,\n",
       " 'alsohttpstcolxw3q0qhcb': 1,\n",
       " 'httpstco3mk25xflup': 1,\n",
       " 'had': 1,\n",
       " 'biggest': 1,\n",
       " 'carb': 1,\n",
       " 'lunch': 1,\n",
       " 'sies': 1,\n",
       " 'dixonjrdc': 1,\n",
       " 'skynewsaust': 1,\n",
       " 'murraywatt': 1,\n",
       " 'scottmorrisonmp': 1,\n",
       " 'peterduttonmp': 1,\n",
       " 'least': 1,\n",
       " \"wasn't\": 1,\n",
       " 'cafe': 1,\n",
       " 'terrorist': 1,\n",
       " 'p…': 1,\n",
       " 'prodeegy': 1,\n",
       " 'vagina': 1,\n",
       " 'cleans': 1,\n",
       " 'itself': 1,\n",
       " 'yes': 1,\n",
       " 'doesn’t': 1,\n",
       " 'mean': 1,\n",
       " 'shouldn’t': 1,\n",
       " 'wash': 1,\n",
       " 'care': 1,\n",
       " 'aunty': 1,\n",
       " 'olo…': 1,\n",
       " 'brakweku3': 1,\n",
       " 'always': 1,\n",
       " 'forgiving': 1,\n",
       " 'even': 1,\n",
       " 'if': 1,\n",
       " 'broke': 1,\n",
       " 'patience': 1,\n",
       " 'shall': 1,\n",
       " 'provide': 1,\n",
       " 'than': 1,\n",
       " '100kit': 1,\n",
       " 'intro': 1,\n",
       " 'full': 1,\n",
       " 'report': 1,\n",
       " 'httpstcoq6oog9zc2b': 1,\n",
       " 'helps': 1,\n",
       " 'explain': 1,\n",
       " 'why': 1,\n",
       " 'challenges': 1,\n",
       " 'remains': 1,\n",
       " 'attractive…': 1,\n",
       " 'these': 1,\n",
       " 'melted': 1,\n",
       " 'resolidified': 1,\n",
       " 'any': 1,\n",
       " 'chance': 1,\n",
       " '😬': 1,\n",
       " 'httpstco5y6vj0mggg': 1,\n",
       " 'senzoncanana': 1,\n",
       " 'ghost': 1,\n",
       " \"don't\": 1,\n",
       " 'stay': 1,\n",
       " 'wherever': 1,\n",
       " 'tf': 1,\n",
       " 'gone': 1,\n",
       " 'steventaylorsa': 1,\n",
       " 'lindtsa': 1,\n",
       " 'vehicle': 1,\n",
       " 'person': 1,\n",
       " 'drove': 1,\n",
       " 'reckless': 1,\n",
       " 'almost': 1,\n",
       " 'run': 1,\n",
       " 'road': 1,\n",
       " 'market': 1,\n",
       " 'vehic…': 1,\n",
       " 'learn': 1,\n",
       " 'eyebrows': 1,\n",
       " '😳': 1,\n",
       " 'atribecalledl': 1,\n",
       " 'gif': 1,\n",
       " 'everything': 1,\n",
       " 'looool': 1,\n",
       " 'httpstcopsy4oo75om': 1,\n",
       " 'silkyandsweet87': 1,\n",
       " 'watching': 1,\n",
       " 'rudolph': 1,\n",
       " 'valentino': 1,\n",
       " 'movie': 1,\n",
       " 'httpstcodcpoh9wfpc': 1,\n",
       " 'thinking': 1,\n",
       " 'she': 1,\n",
       " 'caught': 1,\n",
       " 'offguard': 1,\n",
       " 'nibahle': 1,\n",
       " 'nonetheless': 1,\n",
       " 'casskhaw': 1,\n",
       " '1st': 1,\n",
       " 'choice': 1,\n",
       " '150g': 1,\n",
       " 'cadbury': 1,\n",
       " 'milk': 1,\n",
       " '2nd': 1,\n",
       " 'fancy': 1,\n",
       " 'artisanal': 1,\n",
       " 'stuff': 1,\n",
       " 'provided': 1,\n",
       " 'resist': 1,\n",
       " 'temptation': 1,\n",
       " 'load': 1,\n",
       " 'nonchocolate': 1,\n",
       " 'extras': 1,\n",
       " '3': 1,\n",
       " 'premium': 1,\n",
       " 'brand': 1,\n",
       " '4': 1,\n",
       " 'dept': 1,\n",
       " '4th': 1,\n",
       " 'gross': 1,\n",
       " 'brown': 1,\n",
       " 'wax': 1,\n",
       " 'eggs': 1,\n",
       " 'liragalore': 1,\n",
       " 'he’s': 1,\n",
       " 'nigga': 1,\n",
       " 'bitch': 1,\n",
       " 'httpstcoxl9upyzkw2': 1,\n",
       " 'thevortex794': 1,\n",
       " 'liked': 1,\n",
       " 'youtube': 1,\n",
       " 'video': 1,\n",
       " 'httpstcoknpc7tzusb': 1,\n",
       " 'evenings': 1,\n",
       " 'masterath': 1,\n",
       " '17': 1,\n",
       " \"lindt's\": 1,\n",
       " 'stand': 1,\n",
       " 'campinas': 1,\n",
       " 'sp': 1,\n",
       " 'httpstcodrzbdj2tzm': 1,\n",
       " '🤤': 1,\n",
       " '‘em': 1,\n",
       " '💥': 1,\n",
       " 'httpstcoxxkjnczh29': 1,\n",
       " 'bosspaul72': 1,\n",
       " 'dewsburyrock': 1,\n",
       " 'ohhh': 1,\n",
       " 'sounds': 1,\n",
       " 'interestingi': 1,\n",
       " 'saw': 1,\n",
       " 'did': 1,\n",
       " 'hot': 1,\n",
       " 'lack': 1,\n",
       " 'caffeine': 1,\n",
       " 'meant': 1,\n",
       " 'tonight': 1,\n",
       " 'talk': 1,\n",
       " 'feelings': 1,\n",
       " 'listen': 1,\n",
       " \"won't\": 1,\n",
       " 'judge': 1,\n",
       " 'give': 1,\n",
       " 'say': 1,\n",
       " 'understand': 1,\n",
       " 'lindz': 1,\n",
       " 'ok': 1,\n",
       " 'feel': 1,\n",
       " 'bought': 1,\n",
       " 'giving': 1,\n",
       " 'ass': 1,\n",
       " 'httpstcomfq1htxawr': 1,\n",
       " 'kat2118': 1,\n",
       " 'well': 1,\n",
       " 'call': 1,\n",
       " 'labs': 1,\n",
       " 'i’d': 1,\n",
       " 'mine': 1,\n",
       " 'quietly': 1,\n",
       " '😜': 1,\n",
       " 'ngilibhubesi': 1,\n",
       " 'wonisa': 1,\n",
       " 'bantfu': 1,\n",
       " 'lo': 1,\n",
       " 'melodydominic': 1,\n",
       " 'eyeslove': 1,\n",
       " 'em': 1,\n",
       " 'couldshow': 1,\n",
       " 'ie': 1,\n",
       " 'actions': 1,\n",
       " 'words': 1,\n",
       " 'greatestdebt': 1,\n",
       " 'ruthsutoye': 1,\n",
       " '26': 1,\n",
       " 'stroke': 1,\n",
       " 'week': 1,\n",
       " 'caused': 1,\n",
       " 'stress': 1,\n",
       " 'anxiety': 1,\n",
       " 'ourselves': 1,\n",
       " 'wandidesigns': 1,\n",
       " 'anyone': 1,\n",
       " 'experience': 1,\n",
       " 'total': 1,\n",
       " 'bliss': 1,\n",
       " 'muting': 1,\n",
       " 'tele': 1,\n",
       " 'putting': 1,\n",
       " 'phone': 1,\n",
       " 'down': 1,\n",
       " 'ignoring': 1,\n",
       " 'senses': 1,\n",
       " 'absorb': 1,\n",
       " 'sucker': 1,\n",
       " 'craving': 1,\n",
       " 'oreo': 1,\n",
       " 'blizzard': 1,\n",
       " 'pineapple': 1,\n",
       " 'pizza': 1,\n",
       " 'around': 1,\n",
       " 'town': 1,\n",
       " 'subway': 1,\n",
       " 'krispy': 1,\n",
       " 'kreme': 1,\n",
       " 'donuts': 1,\n",
       " 'reese': 1,\n",
       " 'pb': 1,\n",
       " 'sticks': 1,\n",
       " 'etc': 1,\n",
       " \"wouldn't\": 1,\n",
       " 'mind': 1,\n",
       " 'bedroom': 1,\n",
       " 'httpstcojugqkpjzmk': 1,\n",
       " 'swiss': 1,\n",
       " 'heaven': 1,\n",
       " 'httpstcocrj4jfx90y': 1,\n",
       " 'shocking': 1,\n",
       " 'really': 1,\n",
       " 'httpstcoo92fd0ur67': 1,\n",
       " 'miraslama': 1,\n",
       " 'white': 1,\n",
       " 'fuckcheey': 1,\n",
       " 'him': 1,\n",
       " 'i’m': 1,\n",
       " 'outside': 1,\n",
       " 'dumb': 1,\n",
       " 'wrong': 1,\n",
       " 'house': 1,\n",
       " 'httpstcoelo4216oaz': 1,\n",
       " 'goal': 1,\n",
       " 'leave': 1,\n",
       " 'south': 1,\n",
       " 'africa': 1,\n",
       " 'amazonin': 1,\n",
       " 'cloudtailindia': 1,\n",
       " 'planning': 1,\n",
       " '50': 1,\n",
       " 'boxed': 1,\n",
       " 'assorted': 1,\n",
       " 'boxes': 1,\n",
       " 'bulk': 1,\n",
       " 'buying': 1,\n",
       " 'team': 1,\n",
       " 'reach': 1,\n",
       " 'aspect': 1,\n",
       " 'amazonhelp': 1,\n",
       " 'mgmpas': 1,\n",
       " 'lime': 1,\n",
       " 'indeed': 1,\n",
       " 'lovely': 1,\n",
       " '😀': 1,\n",
       " 'snapjiggar': 1,\n",
       " 'most': 1,\n",
       " 'become': 1,\n",
       " 'expensive': 1,\n",
       " 'difficult': 1,\n",
       " 'vs': 1,\n",
       " 'eating': 1,\n",
       " 'iantony5': 1,\n",
       " 'rejoice': 1,\n",
       " 'die': 1,\n",
       " 'hands': 1,\n",
       " 'children': 1,\n",
       " 'thanosbe': 1,\n",
       " 'thankful': 1,\n",
       " 'meaningless': 1,\n",
       " 'lives…': 1,\n",
       " 'grateful': 1,\n",
       " 'rush': 1,\n",
       " 'push': 1,\n",
       " 'achieve': 1,\n",
       " 'certain': 1,\n",
       " 'age': 1,\n",
       " 'finest': 1,\n",
       " 'joydividing': 1,\n",
       " 'remortgage': 1,\n",
       " 'thickleeyonce': 1,\n",
       " 'intentions': 1,\n",
       " 'worth': 1,\n",
       " 'raven80504432': 1,\n",
       " 'pheuk': 1,\n",
       " 'draseemmalhotra': 1,\n",
       " 'obvious': 1,\n",
       " 'carbs': 1,\n",
       " 'eat': 1,\n",
       " 'choc': 1,\n",
       " 'maybe': 1,\n",
       " '2': 1,\n",
       " 'squares': 1,\n",
       " '1g': 1,\n",
       " 'per': 1,\n",
       " 'rest': 1,\n",
       " 'incidental': 1,\n",
       " 'veg': 1,\n",
       " 'cereals': 1,\n",
       " 'seed': 1,\n",
       " 'oils': 1,\n",
       " 'mayo': 1,\n",
       " 'httpstcoxcxa8y7vly': 1,\n",
       " 'quietnqueer': 1,\n",
       " 'psa': 1,\n",
       " 'everyone': 1,\n",
       " 'following': 1,\n",
       " 'abusers': 1,\n",
       " 'immediately': 1,\n",
       " 'unfollow': 1,\n",
       " 'block': 1,\n",
       " 'httpstco…': 1,\n",
       " 'therealkyyla': 1,\n",
       " '30': 1,\n",
       " 'minutes': 1,\n",
       " 'vent': 1,\n",
       " 'wished': 1,\n",
       " 'kept': 1,\n",
       " 'thoughts': 1,\n",
       " 'myself': 1,\n",
       " 'hermainem': 1,\n",
       " 'goodnight': 1,\n",
       " 'single': 1,\n",
       " 'virgins': 1,\n",
       " '💖': 1,\n",
       " '💕': 1,\n",
       " 'iamsteveolaa': 1,\n",
       " 'husband': 1,\n",
       " 'everytime': 1,\n",
       " 'hit': 1,\n",
       " 'fight': 1,\n",
       " 'manage': 1,\n",
       " 'anger': 1,\n",
       " 'wife': 1,\n",
       " 'clean': 1,\n",
       " 'toilet': 1,\n",
       " 'seat': 1,\n",
       " 'hu…': 1,\n",
       " 'kwanelejay': 1,\n",
       " 'leaving': 1,\n",
       " 'lowest': 1,\n",
       " 'probably': 1,\n",
       " 'selfish': 1,\n",
       " 'an…': 1,\n",
       " '2019africa': 1,\n",
       " 'idea': 1,\n",
       " 'wats': 1,\n",
       " 'missxo6': 1,\n",
       " 'girl': 1,\n",
       " 'street': 1,\n",
       " '22': 1,\n",
       " 'she’s': 1,\n",
       " 'pregnant': 1,\n",
       " 'then': 1,\n",
       " 'tiny': 1,\n",
       " 'voice': 1,\n",
       " 'inside': 1,\n",
       " 'said': 1,\n",
       " 'ungenaphi': 1,\n",
       " 'rkm17': 1,\n",
       " 'stevedodd': 1,\n",
       " 'primebane': 1,\n",
       " 'include': 1,\n",
       " 'few': 1,\n",
       " 'segments': 1,\n",
       " 'cent': 1,\n",
       " 'good': 1,\n",
       " '170': 1,\n",
       " 'calories': 1,\n",
       " 'help': 1,\n",
       " 'cravings': 1,\n",
       " 'strict': 1,\n",
       " 'limit': 1,\n",
       " 'nessanessx': 1,\n",
       " 'being': 1,\n",
       " 'patient': 1,\n",
       " 'understanding': 1,\n",
       " 'those': 1,\n",
       " 'afraid': 1,\n",
       " 'fault': 1,\n",
       " 'own': 1,\n",
       " 'httpstcoui4…': 1,\n",
       " 'high': 1,\n",
       " 'quality': 1,\n",
       " 'online': 1,\n",
       " 'australia': 1,\n",
       " 'httpstcomi7yze1on1': 1,\n",
       " 'httpstco4vwpu5rucz': 1,\n",
       " 'joecouk': 1,\n",
       " 'live': 1,\n",
       " 'extravagant': 1,\n",
       " 'spread': 1,\n",
       " 'httpstcolvylmueqs0': 1,\n",
       " 'cateholder': 1,\n",
       " 'mikecarlton01': 1,\n",
       " 'chap': 1,\n",
       " 'originally': 1,\n",
       " 'victim': 1,\n",
       " 'circumstances': 1,\n",
       " 'were': 1,\n",
       " 'making': 1,\n",
       " 'which': 1,\n",
       " 'led': 1,\n",
       " 'further': 1,\n",
       " 'rometheblack': 1,\n",
       " '🙄': 1,\n",
       " 'everyday': 1,\n",
       " 'favourite': 1,\n",
       " 'flavour': 1,\n",
       " 'excellence': 1,\n",
       " 'httpstco1pu2zjtilv': 1,\n",
       " 'theobileonardo': 1,\n",
       " 'mentioned': 1,\n",
       " 'big': 1,\n",
       " 'gerarof': 1,\n",
       " 'ma': 1,\n",
       " 'fiiloenkoane': 1,\n",
       " 'somesaylola': 1,\n",
       " 'essenceza': 1,\n",
       " 'congratulations': 1,\n",
       " '🌹': 1,\n",
       " '💚': 1,\n",
       " 'entered': 1,\n",
       " 'win': 1,\n",
       " 'ebook': 1,\n",
       " 'match': 1,\n",
       " 'heavenopen': 1,\n",
       " 'int': 1,\n",
       " 'httpstco9v5afmxrsu': 1,\n",
       " 'kirkland': 1,\n",
       " 'boneless': 1,\n",
       " 'skinless': 1,\n",
       " 'chicken': 1,\n",
       " 'thighs': 1,\n",
       " 'wish': 1,\n",
       " 'easy': 1,\n",
       " 'tuned': 1,\n",
       " '24': 1,\n",
       " 'ukuleles': 1,\n",
       " 'paid': 1,\n",
       " 'guess': 1,\n",
       " 'professional': 1,\n",
       " 'musician': 1,\n",
       " 'now': 1,\n",
       " 'tonikroos': 1,\n",
       " 'prefer': 1,\n",
       " 'httpstcoxgob2dj611': 1,\n",
       " 'nkaniyabathathu': 1,\n",
       " '😇': 1,\n",
       " 'realking101': 1,\n",
       " 'females': 1,\n",
       " 'forever': 1,\n",
       " 'forcing': 1,\n",
       " 'men': 1,\n",
       " 'read': 1,\n",
       " 'bible': 1,\n",
       " '😒': 1,\n",
       " 'nothandom': 1,\n",
       " 'ham': 1,\n",
       " 'mushroom': 1,\n",
       " 'quiche': 1,\n",
       " 'orgasmic': 1,\n",
       " 'nomzytwits': 1,\n",
       " 'starlingvibe': 1,\n",
       " 'bad': 1,\n",
       " 'our': 1,\n",
       " 'crushes': 1,\n",
       " 'crush': 1,\n",
       " 'better': 1,\n",
       " 'relationship': 1,\n",
       " 'told': 1,\n",
       " 'pout': 1,\n",
       " \"she's\": 1,\n",
       " 'cute': 1,\n",
       " 'omg': 1,\n",
       " 'httpstconmxpu9rumc': 1,\n",
       " 'memetribute': 1,\n",
       " '100': 1,\n",
       " 'accurate': 1,\n",
       " 'httpstcotup6g14vyq': 1,\n",
       " 'balls': 1,\n",
       " 'dead': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'often?\\nthat’s'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-484c0451e6eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtw_limpo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Relevante\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mir_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mr_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'often?\\nthat’s'"
     ]
>>>>>>> 1de7c95de62bafdedbb1ea2e99743d71d6ac761d
    }
   ],
   "source": [
    "#adicionou as palavras no dicionário de frequencia\n",
    "r_freq = {}\n",
    "ir_freq = {}\n",
    "\n",
    "for palavras in pala:\n",
    "    r_freq[palavras] = 1\n",
    "    ir_freq[palavras] = 1\n",
    "\n",
    "for g in range(len(tw_limpo)):\n",
    "    linha = tw_limpo[\"Treinamento\"][g].split(' ')\n",
<<<<<<< HEAD
    "    for v in range(len(linha)):\n",
    "        if tw_limpo[\"Relevante\"][v] == 0:\n",
    "            ir_freq[palavra] += 1\n",
    "        else:\n",
    "            r_freq[palavra] += 1\n",
    "                \n",
    "ir_freq"
=======
    "    for v in linha:\n",
    "        if tw_limpo[\"Relevante\"][g] == 0:\n",
    "            ir_freq[v] += 1\n",
    "        else:\n",
    "            r_freq[v] += 1"
>>>>>>> 1de7c95de62bafdedbb1ea2e99743d71d6ac761d
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probabilidade de ser relevante da cada palavra"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 11,
>>>>>>> 1de7c95de62bafdedbb1ea2e99743d71d6ac761d
   "metadata": {},
   "outputs": [],
   "source": [
    "r_prob = {}\n",
    "ir_prob = {}\n",
    "\n",
    "#pala é a lista de palavras dos tweets\n",
    "\n",
    "for palavra in pala:\n",
    "    r_prob[palavra] = r_freq[palavra]/(len(palavra)+Rel)\n",
    "    ir_prob[palavra] = ir_freq[palavra]/(len(palavra)+Irre)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(relevante) e P(irrelevante)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 15,
>>>>>>> 1de7c95de62bafdedbb1ea2e99743d71d6ac761d
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5633333333333334"
      ]
     },
<<<<<<< HEAD
     "execution_count": 10,
=======
     "execution_count": 15,
>>>>>>> 1de7c95de62bafdedbb1ea2e99743d71d6ac761d
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrelevante = 0\n",
    "relevante = 0 \n",
    "for m in tw_limpo['Relevante']:\n",
    "    if m == 0:\n",
    "        irrelevante += 1\n",
    "    else:\n",
    "        relevante += 1 \n",
    "\n",
    "Pi = irrelevante/len(tw_limpo['Relevante'])\n",
    "Pr = relevante/len(tw_limpo['Relevante'])\n",
    "Pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teoria de Naive - Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Frase|relevante) = P(palavra|relevante) * P(outra_paravra|relevante)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-4c4cefd7ff81>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-4c4cefd7ff81>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
=======
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Teste'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Teste'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1234bafc7b3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtw_limpo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Teste'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mfrase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfrase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Teste'"
>>>>>>> 1de7c95de62bafdedbb1ea2e99743d71d6ac761d
     ]
    }
   ],
   "source": [
    "mult = 1\n",
    "\n",
    "for i in tw_limpo['Treinamento']:\n",
    "    frase = i.split() \n",
    "    for palavra in frase:\n",
    "        if palavra in r_prob[palavra]:\n",
    "            mult = mult * r_prob[palavra]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudança de índice - Bia fez\n",
    "\n",
    "A mudança de iíndice foi realizada para que a primeira coluna do DataFrame fossem os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_idx = tw.set_index('Treinamento')\n",
    "tw_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separando Relevante e Irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = tw_idx[tw_idx.Relevante == 1]\n",
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrel = tw_idx[tw_idx.Relevante == 0]\n",
    "irrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = tw.Treinamento.tolist()\n",
    "print (lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando as listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = tw.groupby('Relevante').apply(lambda grupo: grupo.Treinamento.tolist()).tolist()\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_r = []\n",
    "lista_r.append(lista[1])\n",
    "lista_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_ir = []\n",
    "lista_ir.append(lista[0])\n",
    "lista_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_rel = []\n",
    "for e in lista_r[0]:\n",
    "    strings = e.split(' ')\n",
    "    lista_rel.append(strings)\n",
    "    \n",
    "lista_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_irrel = []\n",
    "for i in lista_ir[0]:\n",
    "    string = i.split(' ')\n",
    "    lista_irrel.append(string)\n",
    "    \n",
    "    \n",
    "lista_irrel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referências\n",
    "* To list: https://pt.stackoverflow.com/questions/243921/criar-lista-com-conte%C3%BAdo-de-colunas\n",
    "\n",
    "* Split: https://www.tutorialspoint.com/python/string_split.htm\n",
    "* Tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
